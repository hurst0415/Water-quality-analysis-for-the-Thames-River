{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import math\n",
    "import string\n",
    "import sdeint\n",
    "import random\n",
    "import statistics\n",
    "import scipy.signal\n",
    "import scipy.stats as st\n",
    "\n",
    "from scipy.stats import lognorm, expon, rv_continuous, kurtosis, ttest_ind\n",
    "from scipy import stats,optimize\n",
    "from scipy.integrate import odeint\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import gamma, digamma\n",
    "\n",
    "from joblib import Parallel, delayed \n",
    "from matplotlib import gridspec\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from itertools import accumulate\n",
    "from collections import Counter\n",
    "from datetime import datetime,timedelta\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "import os\n",
    "import elevation\n",
    "#from PyEMD import EMD\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read\n",
    "TBB_data = pd.read_csv('Dataframes_new_new/TBB_data.csv')\n",
    "TKB_data = pd.read_csv('Dataframes_new_new/TKB_data.csv')\n",
    "TChP_data = pd.read_csv('Dataframes_new_new/TChP_data.csv')\n",
    "TH_data = pd.read_csv('Dataframes_new_new/TH_data.csv')\n",
    "TPur_data = pd.read_csv('Dataframes_new_new/TPur_data.csv')\n",
    "TPut_data = pd.read_csv('Dataframes_new_new/TPut_data.csv')\n",
    "TCaP_data = pd.read_csv('Dataframes_new_new/TCaP_data.csv')\n",
    "TBGP_data = pd.read_csv('Dataframes_new_new/TBGP_data.csv')\n",
    "TEB_data = pd.read_csv('Dataframes_new_new/TEB_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_names = ['TBB_data', 'TKB_data', 'TChP_data', 'TH_data', 'TPur_data', 'TPut_data', 'TCaP_data', 'TBGP_data', 'TEB_data']\n",
    "day = 24*60*60 #seconds\n",
    "year = (365.2425)*day\n",
    "\n",
    "for name in dataframe_names:\n",
    "    # Read the csv file\n",
    "    df = pd.read_csv('Dataframes_new_new/'+name + '.csv')\n",
    "    df = pd.read_csv('Dataframes_new_new/'+name + '.csv')\n",
    "    df.pop('DATE')\n",
    "    df.pop('TIME')\n",
    "    df.pop('Hour of Day')\n",
    "    df.pop('Month')\n",
    "    df.pop('Day of Week')\n",
    "    df.pop('dist_to_sea (km)')\n",
    "    df.pop('DOO (percent saturation) Class')\n",
    "    df = df.dropna()\n",
    " \n",
    "    date_time = df['DATE TIME']\n",
    "    date_time = pd.to_datetime(date_time, format='%Y-%m-%d %H:%M:%S')\n",
    "    timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
    "    \n",
    "    #returns component of each datetime to range from -0.5 to 0.5\n",
    "    df['Hour of Day'] = date_time.dt.hour/23-0.5\n",
    "    df['Day of Week'] = date_time.dt.dayofweek/6-0.5\n",
    "    df['Month of Year'] = (date_time.dt.month-1)/11-0.5\n",
    "\n",
    "    df['Half Day sin'] = np.sin(timestamp_s * (2 * np.pi / (day/2)))\n",
    "    df['Half Day cos'] = np.cos(timestamp_s * (2 * np.pi / (day/2)))\n",
    "\n",
    "    df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "    df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))  \n",
    "    \n",
    "    if name in ['TBB_data', 'TKB_data', 'TH_data','TChP_data']:     \n",
    "        df.pop('OB_END_TIME')    \n",
    "    #include TPut\n",
    "    if name in ['TBGP_data', 'TEB_data']:     \n",
    "        df.pop('OB_DATE')   \n",
    "        \n",
    "    if name in ['TBB_data', 'TKB_data', 'TH_data', 'TPut_data', 'TCaP_data', 'TBGP_data', 'TEB_data']:     \n",
    "        #Salinity same as EC\n",
    "        df.pop('SALINITY')\n",
    "\n",
    "    df.to_csv('Dataframes_new/'+name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a string is NAN   \n",
    "def isNaN(string):\n",
    "    return string != string\n",
    "#path\n",
    "path_sites = 'E:\\\\PHD\\Water\\EA'\n",
    "\n",
    "#TEB_data['DATE TIME'] = TEB_data['DATE TIME'].apply(pd.to_datetime)\n",
    "#new_col = TEB_data['DATE TIME'].dt.month \n",
    "#TEB_data.insert(loc=11, column='Month', value=new_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to a date change in path_sites+ ..csv, the current doesn't correctly reflect the datasets we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Thames Barrier Gardens Pier data\n",
    "TBGP_data=pd.read_csv(path_sites+'\\BARIERA.csv')\n",
    "#remove zeros\n",
    "TBGP_data[['TEMP','COND','PH','AMMONIUM','TURBIDITY','DOO','DOO-MGL','BATTERY','SALINTY']]=\\\n",
    "TBGP_data[['TEMP','COND','PH','AMMONIUM','TURBIDITY','DOO','DOO-MGL','BATTERY','SALINTY']].replace(0,np.nan)\n",
    "#remove negatives\n",
    "TBGP_data[TBGP_data[['COND','PH','AMMONIUM','DOO','DOO-MGL','BATTERY','SALINTY']]<0] = np.nan\n",
    "#filter corrupted data\n",
    "TBGP_data['AMMONIUM'][0:20950] = np.nan\n",
    "TBGP_data['AMMONIUM'][32353:35327] = np.nan\n",
    "TBGP_data['AMMONIUM'][146103:] = np.nan\n",
    "\n",
    "#Read Thames Brentford Badge data\n",
    "TBB_data=pd.read_csv(path_sites+'\\BREPON.csv')\n",
    "TBB_data[['TEMP','COND','PH','AMMONIUM','TURBIDITY','DOO','DOO-MGL','BATTERY','SALINITY']]=\\\n",
    "TBB_data[['TEMP','COND','PH','AMMONIUM','TURBIDITY','DOO','DOO-MGL','BATTERY','SALINITY']].replace(0,np.nan)\n",
    "TBB_data[TBB_data[['COND','PH','AMMONIUM','DOO','DOO-MGL','BATTERY','SALINITY']]<0] = np.nan\n",
    "#Filter out EC<10 due to sudden dropout\n",
    "TBB_data['COND'][TBB_data['COND']<10] = np.nan\n",
    "\n",
    "#Read Thames Cadogan Pier data\n",
    "TCaP_data=pd.read_csv(path_sites+'\\CADOG2.csv')\n",
    "TCaP_data[['TEMP','COND','PH','AMMONIUM','TURBIDITY','DOO','DOO-MGL','BATTERY','SALINITY']]=\\\n",
    "TCaP_data[['TEMP','COND','PH','AMMONIUM','TURBIDITY','DOO','DOO-MGL','BATTERY','SALINITY']].replace(0,np.nan)\n",
    "TCaP_data[TCaP_data[['COND','PH','AMMONIUM','DOO','DOO-MGL','BATTERY','SALINITY']]<0] = np.nan\n",
    "\n",
    "#Read Thames Chiswick Pier data\n",
    "TChP_data=pd.read_csv(path_sites+'\\GPRSD8A.csv')\n",
    "TChP_data[['TEMP','COND','PH','AMMONIUM','TURBIDITY','DOO','DOO-MGL','BATTERY']]=\\\n",
    "TChP_data[['TEMP','COND','PH','AMMONIUM','TURBIDITY','DOO','DOO-MGL','BATTERY']].replace(0,np.nan)\n",
    "TChP_data[TChP_data[['COND','PH','AMMONIUM','DOO','DOO-MGL','BATTERY']]<0] = np.nan\n",
    "#Filter out faulty data\n",
    "TChP_data['DOO'][91647:94328]=np.nan \n",
    "TChP_data['DOO-MGL'][91647:94328]=np.nan \n",
    "\n",
    "#Read Thames Erith Badge data\n",
    "TEB_data=pd.read_csv(path_sites+'\\ERITH1.csv')\n",
    "TEB_data[['TEMP','COND','PH','TURBIDITY','DOO','DOO-MGL','BATTERY','SALINITY']]=\\\n",
    "TEB_data[['TEMP','COND','PH','TURBIDITY','DOO','DOO-MGL','BATTERY','SALINITY']].replace(0,np.nan)\n",
    "TEB_data[TEB_data[['COND','PH','DOO','DOO-MGL','BATTERY','SALINITY']]<0] = np.nan\n",
    "#Some EC measurements in ms unit, change to us\n",
    "TEB_data['COND'][0:30860] = TEB_data['COND'][0:30860]*1000\n",
    "TEB_data['COND'][33844:89695] = TEB_data['COND'][33844:89695]*1000\n",
    "TEB_data['COND'][89992:] = TEB_data['COND'][89992:]*1000\n",
    "#remove hourly measurement\n",
    "TEB_data = TEB_data[3371:]\n",
    "#remove doo-mgl sudden fluctuations\n",
    "TEB_data['DOO-MGL'][21846:21868] = np.nan\n",
    "TEB_data = TEB_data[TEB_data['DOO-MGL']<25]\n",
    "\n",
    "#Read Thames Hammersmith data\n",
    "TH_data=pd.read_csv(path_sites+'\\HAMME2.csv')\n",
    "TH_data[['TEMP','COND','PH','AMMONIUM','TURBIDITY','DOO','DOO-MGL','BATTERY','SALINITY']]=\\\n",
    "TH_data[['TEMP','COND','PH','AMMONIUM','TURBIDITY','DOO','DOO-MGL','BATTERY','SALINITY']].replace(0,np.nan)\n",
    "TH_data[TH_data[['COND','PH','AMMONIUM','DOO','DOO-MGL','BATTERY','SALINITY']]<0] = np.nan\n",
    "#data failure before sonde change\n",
    "TH_data['AMMONIUM'][93935:99845] =np.nan\n",
    "#Filter EC huge spikes:\n",
    "TH_data['COND'][117335:119522] =np.nan\n",
    "TH_data['COND'][119723:120966] =np.nan\n",
    "TH_data['COND'][121014:121150] =np.nan\n",
    "TH_data['COND'][121179:121253] =np.nan\n",
    "TH_data['COND'][121282:121287] =np.nan\n",
    "TH_data['COND'][121326:121412] =np.nan\n",
    "TH_data['COND'][121517:121752] =np.nan\n",
    "\n",
    "#Read Thames Kew Badge data\n",
    "TKB_data=pd.read_csv(path_sites+'\\KEWPON.csv')\n",
    "TKB_data[['TEMP','COND','PH','AMMONIUM','TURBIDITY','DOO','DOO-MGL','BATTERY','SALINITY']]=\\\n",
    "TKB_data[['TEMP','COND','PH','AMMONIUM','TURBIDITY','DOO','DOO-MGL','BATTERY','SALINITY']].replace(0,np.nan)\n",
    "TKB_data[TKB_data[['COND','PH','AMMONIUM','DOO','DOO-MGL','BATTERY','SALINITY']]<0] = np.nan\n",
    "#Ammonium 9999, hence remove        \n",
    "TKB_data['AMMONIUM'][76514:76515] = np.nan\n",
    "#Filter faulty data\n",
    "TKB_data['AMMONIUM'][76536:76865] = np.nan\n",
    "#AMMONIUM sonde probe fault:\n",
    "TKB_data['AMMONIUM'][164299:166555] = np.nan\n",
    "#Erith sonde used:\n",
    "TKB_data['AMMONIUM'][50637:51688] =np.nan      \n",
    "\n",
    "\n",
    "#Read Thames Purfleet data\n",
    "TPur_data=pd.read_csv(path_sites+'\\E03036A.csv')\n",
    "TPur_data[['TEMP','COND','PH','TURBIDITY','DOO-pcent','DOO-MGL','BATTERY']]=\\\n",
    "TPur_data[['TEMP','COND','PH','TURBIDITY','DOO-pcent','DOO-MGL','BATTERY']].replace(0,np.nan)\n",
    "TPur_data[TPur_data[['COND','PH','DOO-pcent','DOO-MGL','BATTERY']]<0] = np.nan\n",
    "#Filter faulty EC\n",
    "TPur_data['COND'][15487:15489] = np.nan\n",
    "#change unit ms to us\n",
    "TPur_data['COND'] = TPur_data['COND']*1000\n",
    "#remove spiky /sudden drop of doo-mgl\n",
    "TPur_data['DOO-MGL'][106900:107380]=np.nan\n",
    "TPur_data = TPur_data[TPur_data['DOO-MGL']<25]\n",
    "\n",
    "#Read Thames Putney data\n",
    "TPut_data=pd.read_csv(path_sites+'\\PUTNEY.csv')\n",
    "TPut_data[['TEMP','COND','PH','AMMONIUM','TURBIDITY','DOO-pcent','DOO-MGL','BATTERY','SALINITY']]=\\\n",
    "TPut_data[['TEMP','COND','PH','AMMONIUM','TURBIDITY','DOO-pcent','DOO-MGL','BATTERY','SALINITY']].replace(0,np.nan)\n",
    "TPut_data[TPut_data[['COND','PH','AMMONIUM','DOO-pcent','DOO-MGL','BATTERY','SALINITY']]<0] = np.nan\n",
    "#EC faulty due to sonde change        \n",
    "TPut_data['COND'][154307:154480] =np.nan\n",
    "#Ammonium faulty data   \n",
    "TPut_data['AMMONIUM'][154352:154382] =np.nan\n",
    "TPut_data['AMMONIUM'][154479:154480] =np.nan\n",
    "\n",
    "TBB_data['dist_to_sea (km)']=61.39\n",
    "TKB_data['dist_to_sea (km)']=59.96\n",
    "TChP_data['dist_to_sea (km)']=57.78\n",
    "TH_data['dist_to_sea (km)']=56.64\n",
    "TPut_data['dist_to_sea (km)']= 55.43\n",
    "TCaP_data['dist_to_sea (km)']= 51.91\n",
    "TBGP_data['dist_to_sea (km)']= 37.5\n",
    "TEB_data['dist_to_sea (km)']= 28.77\n",
    "TPur_data['dist_to_sea (km)']=23.26\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEB_data['COND'] = np.where(TEB_data['COND']<40000, TEB_data['COND'], np.nan)\n",
    "TBGP_data['COND'] = np.where(TBGP_data['COND']<25000, TBGP_data['COND'], np.nan)\n",
    "TPur_data['COND'] = np.where(TPur_data['COND']<40000, TPur_data['COND'], np.nan)\n",
    "TH_data['COND'] = np.where(TH_data['COND']<5000, TH_data['COND'], np.nan)\n",
    "TChP_data['COND'] = np.where(TChP_data['COND']<5000, TChP_data['COND'], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEB_data = TEB_data[TEB_data['COND']<40000]\n",
    "TBGP_data = TBGP_data[TBGP_data['COND']<25000]\n",
    "TPur_data = TPur_data[TPur_data['COND']<40000]\n",
    "TH_data = TH_data[TH_data['COND']<5000]\n",
    "TChP_data = TChP_data[TChP_data['COND']<5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_names = ['TBB_data', 'TKB_data', 'TChP_data', 'TH_data', 'TPur_data', 'TPut_data', 'TCaP_data', 'TBGP_data', 'TEB_data']\n",
    "\n",
    "for name in dataframe_names:\n",
    "    # Read the csv file\n",
    "    df = pd.read_csv('Dataframes/'+name + '.csv')\n",
    "    df['DATE TIME'] = df['DATE'] + ' ' + df['TIME']\n",
    "    \n",
    "    if name in ['TBGP_data', 'TPut_data']:\n",
    "        # Convert using the '%Y-%m-%d %H:%M:%S' format for these two dataframes\n",
    "        df['DATE TIME'] = df['DATE TIME'].apply(lambda x: pd.to_datetime(x, format='%Y-%m-%d %H:%M:%S'))\n",
    "    else:\n",
    "        # Convert using the '%d/%m/%Y %H:%M:%S' format for the rest of the dataframes\n",
    "        df['DATE TIME'] = df['DATE TIME'].apply(lambda x: pd.to_datetime(x, format='%d/%m/%Y %H:%M:%S'))\n",
    "\n",
    "    df.to_csv('Dataframes/'+name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove doo data affected by the injection of oxygen in August 2022\n",
    "dataframe_names = ['TBB_data', 'TKB_data', 'TChP_data', 'TH_data', 'TPut_data', 'TCaP_data', 'TBGP_data', 'TEB_data']\n",
    "\n",
    "# Set start and end times\n",
    "start_time = pd.to_datetime('2022-08-17 12:00:00')\n",
    "end_time = pd.to_datetime('2022-09-01 00:00:00')\n",
    "\n",
    "for name in dataframe_names:\n",
    "    # Read the csv file\n",
    "    df = pd.read_csv('Dataframes/'+name + '.csv')\n",
    "\n",
    "    # Ensure 'DATE TIME' is in the datetime format\n",
    "    df['DATE TIME'] = pd.to_datetime(df['DATE TIME'])\n",
    "    \n",
    "    # Create a mask for the time period of interest\n",
    "    mask = (df['DATE TIME'] >= start_time) & (df['DATE TIME'] <= end_time)\n",
    "\n",
    "    # Set 'DOO-MGL' to np.nan for the time period\n",
    "    df.loc[mask, 'DOO-MGL'] = np.nan\n",
    "\n",
    "    # Save the modified dataframe back to a new csv file\n",
    "    df.to_csv('Dataframes/'+name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBB_data.to_csv('Dataframes/TBB_data.csv', index=False)\n",
    "TKB_data.to_csv('Dataframes/TKB_data.csv', index=False)\n",
    "TChP_data.to_csv('Dataframes/TChP_data.csv', index=False)\n",
    "TH_data.to_csv('Dataframes/TH_data.csv', index=False)\n",
    "TPur_data.to_csv('Dataframes/TPur_data.csv', index=False)\n",
    "TPut_data.to_csv('Dataframes/TPut_data.csv', index=False)\n",
    "TCaP_data.to_csv('Dataframes/TCaP_data.csv', index=False)\n",
    "TBGP_data.to_csv('Dataframes/TBGP_data.csv', index=False)\n",
    "TEB_data.to_csv('Dataframes/TEB_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
